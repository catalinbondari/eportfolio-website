<!DOCTYPE html>
<html lang="en">

<head>

    <!-- Basic Page Needs
    ================================================== -->
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Blog - Why it’s been revolutionizing NLP
    </title>

    <meta name="description" content="">
    <meta name="author" content="">
    <meta name="keywords" content="">

    <!-- Mobile Specific Metas
    ================================================== -->
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0">
    <meta name="apple-mobile-web-app-capable" content="yes" />

    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,600,700" rel="stylesheet">

    <!-- Favicon
    ================================================== -->
    <link rel="apple-touch-icon" sizes="180x180" href="assets/img/signature-2.png">
    <link rel="icon" type="image/png" sizes="100x100" href="assets/img/signature-2.png">

    <!-- Stylesheets
    ================================================== -->
    <!-- Bootstrap core CSS -->
    <link href="assets/css/bootstrap.min.css" rel="stylesheet">
    <link href="assets/css/ionicons/css/ionicons.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="assets/css/style.css" rel="stylesheet">
    <link href="assets/css/responsive.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body>

    <header id="masthead" class="site-header" data-anchor-target=".hero"
        data-top="background: rgba(255,255,255,100); padding: 30px 0; box-shadow: 0px 0px 20px 6px rgba(0, 0, 0, 0);"
        data-top-bottom="background: rgba(255,255,255,1); padding: 10px 0; box-shadow: 0px 0px 20px 6px rgba(0, 0, 0, 0.2);">
        <nav id="primary-navigation" class="site-navigation">
            <div class="container">
                <div class="navbar-header page-scroll">

                    <button type="button" class="navbar-toggle collapsed" data-target="#portfolio-perfect-collapse"
                        aria-expanded="false">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a href="index.html" class="site-logo"><img src="assets/img/logo.png" alt="logo" class="logo"></a>
                </div><!-- /.navbar-header -->

                <div class="main-menu" id="portfolio-perfect-collapse">

                    <ul class="nav navbar-nav navbar-right">

                        <li class="page-scroll"><a href="index.html#hero">Home</a></li>
                        <li class="page-scroll"><a href="index.html#about">About</a></li>
                        <li class="page-scroll"><a href="index.html#service">Service</a></li>
                        <li class="page-scroll"><a href="index.html#portfolio">Portfolio</a></li>
                        <li class="page-scroll"><a href="index.html#blog">Blog</a></li>
                        <li class="page-scroll"><a href="index.html#media">Media</a></li>
                        <li class="page-scroll"><a href="index.html#more-about-me">More about me</a></li>
                        <li class="page-scroll"><a href="index.html#contact">Contact</a></li>

                    </ul><!-- /.navbar-nav -->

                </div><!-- /.navbar-collapse -->

            </div>
        </nav><!-- /.primary-navigation -->
    </header><!-- /#header -->

    <div id="hero" class="hero">
    </div><!-- /.hero -->

    <main id="main" class="site-main">
        <!-- start section main content -->
        <section id="about" class="site-section section-about text-center blog-single">
            <div class="container" style="margin-bottom: 30px;">
                <img src="assets/img/project/bert-2-img.jpg" alt="signature" class="img-signature">
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-md-6 col-md-offset-3">
                        <h2>Why it’s been revolutionizing NLP</h2>
                        <img src="assets/img/lines.svg" class="img-lines" alt="lines">




                        <article>

                            <div>
                                <div>
                                    <div>
                                        <p data-selectable-paragraph="">BERT, which stands for Bidirectional Encoder
                                            Representations from Transformers, is a language model published in 2018
                                            that achieved state-of-the-art performance on multiple tasks, including
                                            question-answering and language understanding. It not only beat previous
                                            state-of-the-art computational models, but also surpassed human performance
                                            in question-answering.</p>
                                    </div>
                                </div>
                            </div>
                            <div>
                                <div>
                                    <div>
                                        <p data-selectable-paragraph=""><strong>What&rsquo;s BERT? </strong>BERT is a
                                            computational model that converts words into numbers. This process is
                                            crucial because machine learning models take in numbers (not words) as
                                            inputs, so an algorithm that converts words into numbers allows you to train
                                            machine learning models on your originally-textual data.</p>
                                        <figure>
                                            <div tabindex="0" role="button">
                                                <div>
                                                    <picture>
                                                        <source
                                                            srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*p4LFBwyHtCw_Qq9paDampA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*p4LFBwyHtCw_Qq9paDampA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*p4LFBwyHtCw_Qq9paDampA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*p4LFBwyHtCw_Qq9paDampA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*p4LFBwyHtCw_Qq9paDampA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*p4LFBwyHtCw_Qq9paDampA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p4LFBwyHtCw_Qq9paDampA.png 1400w"
                                                            type="image/webp"
                                                            sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" />
                                                        <source
                                                            srcset="https://miro.medium.com/v2/resize:fit:640/1*p4LFBwyHtCw_Qq9paDampA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*p4LFBwyHtCw_Qq9paDampA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*p4LFBwyHtCw_Qq9paDampA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*p4LFBwyHtCw_Qq9paDampA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*p4LFBwyHtCw_Qq9paDampA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*p4LFBwyHtCw_Qq9paDampA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*p4LFBwyHtCw_Qq9paDampA.png 1400w"
                                                            sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"
                                                            data-testid="og" /><img role="presentation"
                                                            src="https://miro.medium.com/v2/resize:fit:700/1*p4LFBwyHtCw_Qq9paDampA.png"
                                                            alt="" width="700" height="281" />
                                                    </picture>
                                                </div>
                                            </div>
                                            <figcaption data-selectable-paragraph="">BERT is a computational model that
                                                converts words into numbers. Image from <a
                                                    href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank"
                                                    rel="noopener ugc nofollow">(Devlin et al., 2019)</a>.</figcaption>
                                        </figure>
                                    </div>
                                </div>
                            </div>
                            <div>
                                <div>
                                    <div>
                                        <p data-selectable-paragraph=""><strong>What&rsquo;s so great about
                                                BERT?</strong> For me, there are three main things that make BERT so
                                            great.</p>
                                        <ul>
                                            <li data-selectable-paragraph="">Number 1: pre-trained on a lot of data.
                                            </li>
                                            <li data-selectable-paragraph="">Number 2: accounts for a word&rsquo;s
                                                context.</li>
                                            <li data-selectable-paragraph="">Number 3: open-source.</li>
                                        </ul>
                                        <p data-selectable-paragraph="">Let&rsquo;s discuss.</p>
                                        <p data-selectable-paragraph=""><strong>#1: BERT is pre-trained on an absurd
                                                amount of data.</strong> The original BERT model comes in two sizes:
                                            BERT-base (trained on BooksCorpus: ~800 million words) and BERT-large
                                            (trained on English Wikipedia: ~ 2,500 million words). Both of these models
                                            have huge training sets! As anyone in the machine learning field knows, the
                                            power of big data is pretty much unbeatable. When you&rsquo;ve seen 2,500
                                            million words, you&rsquo;re going to be pretty good, even on new words. What
                                            this means is that because BERT was pre-trained so well that it can be
                                            applied on small datasets and still have good performance. I&rsquo;ll use an
                                            example here: I recently worked on <a
                                                href="https://towardsdatascience.com/what-are-people-asking-about-covid-19-a-new-question-classification-dataset-adcaeaddcce4"
                                                target="_blank" rel="noopener">a project to develop a COVID-19
                                                question-answering system</a>. I applied BERT-base and got a 58.1%
                                            accuracy across 15 categories without fine-tuning on my dataset.
                                            What&rsquo;s more interesting to me is that the word &ldquo;COVID&rdquo;
                                            wasn&rsquo;t even in BERT&rsquo;s vocabulary, yet it still got pretty high
                                            accuracy.</p>
                                        <p data-selectable-paragraph=""><strong>#2: BERT is able to account for a
                                                word&rsquo;s context.</strong> Previous methods of word-embedding would
                                            return the same vector for a word no matter how it is used, while BERT
                                            returns different vectors for the same word depending on the words around
                                            it. For example, old methods would return the same embedding for
                                            &ldquo;trust&rdquo; in the following examples:</p>
                                        <blockquote>
                                            <p data-selectable-paragraph="">I can&rsquo;t <strong>trust</strong> you.
                                            </p>
                                            <p data-selectable-paragraph="">They have no <strong>trust</strong> left for
                                                their friend.</p>
                                            <p data-selectable-paragraph="">He has a <strong>trust</strong> fund.</p>
                                        </blockquote>
                                        <p data-selectable-paragraph="">On the other hand, BERT accounts for the context
                                            and would return different embeddings for &ldquo;trust&rdquo; because the
                                            word is being used in different contexts. If you can distinguish between
                                            different use-cases for a word, you have more information available, and
                                            your performance will thus probably increase. A related language modeling
                                            method that also accounts for context is <a
                                                href="https://arxiv.org/pdf/1802.05365.pdf" target="_blank"
                                                rel="noopener ugc nofollow">ELMo</a>.</p>
                                        <p data-selectable-paragraph=""><strong>#3: BERT is open-source.</strong> Being
                                            accessible is a big plus. Much work in the machine learning field is being
                                            pushed to be open-source, since open-source code promotes development by
                                            allowing other researchers to easily apply your ideas. BERT&rsquo;s code is
                                            <a href="https://github.com/google-research/bert" target="_blank"
                                                rel="noopener ugc nofollow">published on GitHub</a>, and it includes an
                                            extensive README with in-depth information on how to use the code, which is
                                            really helpful for anyone who&rsquo;s looking to use it.
                                        </p>
                                        <p data-selectable-paragraph="">For me, I think I was able to download a working
                                            model of BERT in a few minutes, and it took probably less than an hour to
                                            write code that let me run it on my own dataset.</p>
                                    </div>
                                </div>
                            </div>
                            <div>
                                <div>
                                    <div>
                                        <p data-selectable-paragraph="">When you combine these three aspects together,
                                            you get an extremely powerful language model that achieves state-of-the-art
                                            performance on big-name datasets like SQuAD, GLUE, and MultiNLI. It&rsquo;s
                                            got some pretty big advantages that make it so powerful and applicable.</p>
                                        <p data-selectable-paragraph="">It&rsquo;s pre-trained on a lot of data, so you
                                            can apply it on your own (probably small) dataset. It&rsquo;s got contextual
                                            embeddings, so it&rsquo;s performance will be pretty good. And it&rsquo;s
                                            open source, so you can just download it and use it. It&rsquo;s just so
                                            widely-applicable, and that&rsquo;s why it revolutionized NLP.</p>
                                        <p data-selectable-paragraph="">Researchers at Google (the original creators)
                                            planned to use BERT to understand Google searches and improve the accuracy
                                            of Google&rsquo;s own answering services. Others found that BERT could be
                                            useful for more than just Google searches. BERT seems to promise
                                            improvements in key areas of computational linguistics, including chatbots,
                                            question-answering, summarization, and sentiment detection. BERT&rsquo;s
                                            wide applicability can easily be seen: since its publication just over one
                                            year ago, <a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank"
                                                rel="noopener ugc nofollow">the paper</a> has over 8,500 citations.
                                            Furthermore, submissions to the Association for Computational Linguistics
                                            (ACL) conference, the largest international NLP conference, doubled
                                            following the publication of BERT, from 1,544 submissions in 2018 to 2,905
                                            submissions in 2019.</p>
                                        <p data-selectable-paragraph="">BERT will continue revolutionizing the field of
                                            NLP because it provides an opportunity for high performance on small
                                            datasets for a large range of tasks.</p>
                                    </div>
                                </div>
                            </div>

                            <p class="mb-30">
                                <b>Author: Jerry Wei</b>
                            </p>





                        </article>
                    </div>
                </div>
            </div>
        </section><!-- /.secton-about -->
        <!--  </div> -->
        <!-- start section main content -->

    </main><!-- /#main -->
    <footer id="colophon" class="site-footer">
        <div class="container">
            <div class="row">
                <div class="col-sm-4 col-xs-12">
                    <a class="icon facebook-bg" href="#"><i class="ion-social-github"></i></a>
                    <a class="icon twitter-bg" href="#"><i class="ion-social-linkedin"></i></a>
                    <a class="icon gplus-bg" href="#"><i class="ion-social-whatsapp"></i></a>
                    <a class="icon gplus-bg" href="#"><i class="ion-email"></i></a>
                </div>
                <div class="col-sm-4 col-sm-offset-0 col-xs-6 col-xs-offset-3">
                    <p class="copyright">© Catalin Bondari 2024.</p>
                </div>
                <div class="col-sm-4 col-xs-3">
                    <div class="text-right page-scroll">
                        <a class="icon icon-up-bg" href="#hero"><i class="icon-up"></i></a>
                    </div>
                </div>
            </div>
        </div>

    </footer><!-- /#footer -->


    <!-- Bootstrap core JavaScript
================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
    <script src="assets/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
    <script src="assets/js/skrollr.min.js"></script>
    <script
        src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap-progressbar/0.9.0/bootstrap-progressbar.min.js"></script>
    <script src="assets/js/jquery.countTo.min.js"></script>
    <script src="assets/js/script.js"></script>

</body>

</html>